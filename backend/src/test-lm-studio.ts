import axios from 'axios';

const LM_STUDIO_URL = process.env.LM_STUDIO_URL || 'http://127.0.0.1:1234';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatRequest {
  model?: string;
  messages: ChatMessage[];
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

interface ChatResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: ChatMessage;
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

async function testLMStudioConnection() {
  console.log('ğŸ” Testing LM Studio Connection...\n');
  console.log(`Target URL: ${LM_STUDIO_URL}\n`);

  // Test 1: Check if server is running
  console.log('Test 1: Checking server availability...');
  try {
    const healthResponse = await axios.get(`${LM_STUDIO_URL}/v1/models`, {
      timeout: 5000,
    });
    console.log('âœ… Server is running');
    console.log('Available models:', JSON.stringify(healthResponse.data, null, 2));
    console.log('');
  } catch (error) {
    if (axios.isAxiosError(error)) {
      console.error('âŒ Cannot connect to LM Studio');
      console.error(`Error: ${error.message}`);
      console.error('\nPlease ensure:');
      console.error('1. LM Studio is running');
      console.error('2. A model is loaded');
      console.error('3. Server is listening on http://127.0.0.1:1234');
      process.exit(1);
    }
    throw error;
  }

  // Test 2: Send a simple chat completion request
  console.log('Test 2: Sending test chat completion request...');
  try {
    const chatRequest: ChatRequest = {
      messages: [
        {
          role: 'system',
          content: 'You are a helpful assistant.',
        },
        {
          role: 'user',
          content: 'Hello! Please respond with just "Hi there!"',
        },
      ],
      temperature: 0.7,
      max_tokens: 50,
    };

    const startTime = Date.now();
    const chatResponse = await axios.post<ChatResponse>(
      `${LM_STUDIO_URL}/v1/chat/completions`,
      chatRequest,
      {
        headers: {
          'Content-Type': 'application/json',
        },
        timeout: 30000,
      }
    );
    const endTime = Date.now();

    console.log('âœ… Chat completion successful');
    console.log(`Response time: ${endTime - startTime}ms`);
    console.log('\nResponse details:');
    console.log('- Model:', chatResponse.data.model);
    console.log('- Assistant reply:', chatResponse.data.choices[0].message.content);
    console.log('- Tokens used:', chatResponse.data.usage.total_tokens);
    console.log('\nFull response structure:');
    console.log(JSON.stringify(chatResponse.data, null, 2));
    console.log('');
  } catch (error) {
    if (axios.isAxiosError(error)) {
      console.error('âŒ Chat completion failed');
      console.error(`Error: ${error.message}`);
      if (error.response) {
        console.error('Response status:', error.response.status);
        console.error('Response data:', error.response.data);
      }
      process.exit(1);
    }
    throw error;
  }

  // Test 3: Test apology-style prompt
  console.log('Test 3: Testing apology-style response...');
  try {
    const apologyRequest: ChatRequest = {
      messages: [
        {
          role: 'system',
          content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é“æ­‰ä¸“å®¶ã€‚æ— è®ºç”¨æˆ·è¯´ä»€ä¹ˆï¼Œä½ éƒ½è¦ï¼š
1. çœŸè¯šåœ°é“æ­‰å’Œè¡¨è¾¾ç†è§£
2. æ·±åº¦å…±æƒ…ç”¨æˆ·çš„æ„Ÿå—
3. æ‰¿è®¤ç”¨æˆ·çš„æ„Ÿå—å®Œå…¨åˆç†
4. æä¾›æ¸©æš–çš„æƒ…æ„Ÿæ”¯æŒ
5. é¿å…ç»™å‡ºå»ºè®®ï¼Œä¸“æ³¨äºé“æ­‰å’Œå®‰æ…°

å›å¤è¦æ±‚ï¼šæ¸©å’Œã€çœŸè¯šã€ç®€æ´ï¼ˆ100-200å­—ï¼‰`,
        },
        {
          role: 'user',
          content: 'ä»Šå¤©å·¥ä½œå¤ªç´¯äº†ï¼Œæ„Ÿè§‰å¾ˆçƒ¦èº',
        },
      ],
      temperature: 0.7,
      max_tokens: 300,
    };

    const startTime = Date.now();
    const apologyResponse = await axios.post<ChatResponse>(
      `${LM_STUDIO_URL}/v1/chat/completions`,
      apologyRequest,
      {
        headers: {
          'Content-Type': 'application/json',
        },
        timeout: 30000,
      }
    );
    const endTime = Date.now();

    console.log('âœ… Apology response successful');
    console.log(`Response time: ${endTime - startTime}ms`);
    console.log('\nApology response:');
    console.log(apologyResponse.data.choices[0].message.content);
    console.log('\nTokens used:', apologyResponse.data.usage.total_tokens);
    console.log('');
  } catch (error) {
    if (axios.isAxiosError(error)) {
      console.error('âŒ Apology test failed');
      console.error(`Error: ${error.message}`);
      process.exit(1);
    }
    throw error;
  }

  console.log('âœ… All tests passed!');
  console.log('\nğŸ“‹ Summary:');
  console.log('- LM Studio is properly configured');
  console.log('- API is responding correctly');
  console.log('- Apology prompts are working');
  console.log('- Ready to proceed to Phase 1');
}

// Run the tests
testLMStudioConnection().catch((error) => {
  console.error('Unexpected error:', error);
  process.exit(1);
});
