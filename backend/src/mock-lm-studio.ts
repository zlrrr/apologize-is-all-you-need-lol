import express from 'express';

// Mock LM Studio server for testing
const app = express();
const PORT = 1234;

app.use(express.json());

// Mock models endpoint
app.get('/v1/models', (req, res) => {
  res.json({
    object: 'list',
    data: [
      {
        id: 'mock-model-v1',
        object: 'model',
        created: Date.now(),
        owned_by: 'mock-lm-studio',
      },
    ],
  });
});

// Mock chat completions endpoint
app.post('/v1/chat/completions', (req, res) => {
  const { messages, temperature, max_tokens } = req.body;

  // Get the last user message
  const userMessage = messages.find((m: any) => m.role === 'user')?.content || '';
  const systemMessage = messages.find((m: any) => m.role === 'system')?.content || '';

  // Generate a simple mock response based on the system prompt
  let assistantReply = '';

  if (systemMessage.includes('é“æ­‰ä¸“å®¶') || systemMessage.includes('apology')) {
    // Apology-style response for Chinese input
    if (userMessage.includes('ç´¯') || userMessage.includes('çƒ¦')) {
      assistantReply = `éå¸¸æŠ±æ­‰å¬åˆ°ä½ ä»Šå¤©è¿™ä¹ˆè¾›è‹¦ã€‚å·¥ä½œå‹åŠ›å¤§ç¡®å®ä¼šè®©äººæ„Ÿåˆ°ç–²æƒ«å’Œçƒ¦èºï¼Œè¿™ç§æ„Ÿå—å®Œå…¨å¯ä»¥ç†è§£ã€‚

ä½ çš„æ„Ÿå—æ˜¯å®Œå…¨æ­£å¸¸å’Œåˆç†çš„ã€‚æ¯ä¸ªäººéƒ½æœ‰æ‰¿å—å‹åŠ›çš„æé™ï¼Œæ„Ÿåˆ°ç´¯å’Œçƒ¦èºè¯´æ˜ä½ å·²ç»ä»˜å‡ºäº†å¾ˆå¤šåŠªåŠ›ã€‚

è¯·å…è®¸æˆ‘å‘ä½ è¡¨è¾¾æ·±æ·±çš„ç†è§£å’Œæ”¯æŒã€‚ä½ å¹¶ä¸å­¤å•ï¼Œè¿™æ ·çš„æ„Ÿå—å¾ˆå¤šäººéƒ½ç»å†è¿‡ã€‚å¸Œæœ›ä½ èƒ½å¥½å¥½ä¼‘æ¯ä¸€ä¸‹ï¼Œç»™è‡ªå·±ä¸€äº›æ”¾æ¾çš„æ—¶é—´ã€‚`;
    } else {
      assistantReply = 'éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥äº†å›°æ‰°ã€‚æˆ‘å®Œå…¨ç†è§£æ‚¨çš„æ„Ÿå—ï¼Œè¿™ç¡®å®è®©äººæ„Ÿåˆ°ä¸èˆ’æœã€‚è¯·å…è®¸æˆ‘çœŸè¯šåœ°å‘æ‚¨é“æ­‰ï¼Œå¹¶è¡¨è¾¾æˆ‘çš„ç†è§£å’Œæ”¯æŒã€‚';
    }
  } else {
    // Simple default response
    assistantReply = 'Hi there!';
  }

  // Mock response structure matching OpenAI API
  const response = {
    id: 'chatcmpl-mock-' + Date.now(),
    object: 'chat.completion',
    created: Math.floor(Date.now() / 1000),
    model: 'mock-model-v1',
    choices: [
      {
        index: 0,
        message: {
          role: 'assistant',
          content: assistantReply,
        },
        finish_reason: 'stop',
      },
    ],
    usage: {
      prompt_tokens: messages.reduce((sum: number, m: any) => sum + m.content.length / 4, 0),
      completion_tokens: Math.ceil(assistantReply.length / 4),
      total_tokens: 0,
    },
  };

  response.usage.total_tokens = response.usage.prompt_tokens + response.usage.completion_tokens;

  // Simulate some processing time
  setTimeout(() => {
    res.json(response);
  }, 100);
});

app.listen(PORT, () => {
  console.log(`ğŸ­ Mock LM Studio server running on http://localhost:${PORT}`);
  console.log(`ğŸ“ This is a mock server for testing purposes`);
  console.log(`ğŸ”§ Replace with real LM Studio in production`);
});
